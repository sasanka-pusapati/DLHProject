{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 22:40:44.314934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "#sasanka2@illinois.edu - commented out unused imports\n",
    "#import glove\n",
    "#from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "#from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "from keras.backend import set_session, clear_session, get_session\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dict_of_ner):\n",
    "    temp_data = []\n",
    "    for k, v in sorted(dict_of_ner.items()):\n",
    "        temp = []\n",
    "        for embed in v:\n",
    "            temp.append(embed)\n",
    "        temp_data.append(np.mean(temp, axis = 0)) \n",
    "    return np.asarray(temp_data)\n",
    "\n",
    "def make_prediction_multi_avg(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_multi_avg(predictions, probs, ground_truth, \n",
    "                          \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,\n",
    "                          \n",
    "                          sequence_name, type_of_ner):\n",
    "    \n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "    \n",
    "    result_path = \"results/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def avg_ner_model(layer_name, number_of_unit, embedding_name):\n",
    "\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    else:\n",
    "        input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n",
    "#     x_1 = Dense(256, activation='relu')(input_avg)\n",
    "#     x_1 = Dropout(0.3)(x_1)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    x = keras.layers.Concatenate()([x, input_avg])\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # sasanka2@illinois.edu - Using latest api\n",
    "    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "    logits_regularizer = tf.keras.regularizers.L2(0.01)\n",
    "    \n",
    "    #preds = Dense(1, activation='sigmoid',use_bias=False,\n",
    "    #                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    #              kernel_regularizer=logits_regularizer)(x)\n",
    "    preds = Dense(1, activation='sigmoid',use_bias=False,\n",
    "                  kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                  kernel_regularizer=logits_regularizer)(x)\n",
    "    \n",
    "    \n",
    "    opt = Adam(lr=0.001, decay = 0.01)\n",
    "    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "ner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  GRU\n",
      "Hidden unit:  128\n",
      "Embedding:  word2vec\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 22:41:03.948064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8893\n",
      "Epoch 1: val_loss improved from inf to 0.25479, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 6s 19ms/step - loss: 0.3311 - acc: 0.8896 - val_loss: 0.2548 - val_acc: 0.9119\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2609 - acc: 0.9119\n",
      "Epoch 2: val_loss improved from 0.25479 to 0.24796, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 16ms/step - loss: 0.2609 - acc: 0.9119 - val_loss: 0.2480 - val_acc: 0.9152\n",
      "Epoch 3/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9153\n",
      "Epoch 3: val_loss improved from 0.24796 to 0.24462, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 19ms/step - loss: 0.2458 - acc: 0.9155 - val_loss: 0.2446 - val_acc: 0.9147\n",
      "Epoch 4/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9162\n",
      "Epoch 4: val_loss did not improve from 0.24462\n",
      "233/233 [==============================] - 4s 16ms/step - loss: 0.2368 - acc: 0.9164 - val_loss: 0.2461 - val_acc: 0.9152\n",
      "Epoch 5/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9198\n",
      "Epoch 5: val_loss improved from 0.24462 to 0.24430, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2312 - acc: 0.9197 - val_loss: 0.2443 - val_acc: 0.9161\n",
      "Epoch 6/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9199\n",
      "Epoch 6: val_loss improved from 0.24430 to 0.24427, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2275 - acc: 0.9199 - val_loss: 0.2443 - val_acc: 0.9166\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2250 - acc: 0.9215\n",
      "Epoch 7: val_loss improved from 0.24427 to 0.24227, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2250 - acc: 0.9215 - val_loss: 0.2423 - val_acc: 0.9147\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2213 - acc: 0.9230\n",
      "Epoch 8: val_loss did not improve from 0.24227\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2213 - acc: 0.9230 - val_loss: 0.2424 - val_acc: 0.9161\n",
      "Epoch 9/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9226\n",
      "Epoch 9: val_loss did not improve from 0.24227\n",
      "233/233 [==============================] - 4s 16ms/step - loss: 0.2196 - acc: 0.9227 - val_loss: 0.2424 - val_acc: 0.9166\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2163 - acc: 0.9246\n",
      "Epoch 10: val_loss improved from 0.24227 to 0.24189, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2163 - acc: 0.9246 - val_loss: 0.2419 - val_acc: 0.9156\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2148 - acc: 0.9228\n",
      "Epoch 11: val_loss did not improve from 0.24189\n",
      "233/233 [==============================] - 4s 16ms/step - loss: 0.2148 - acc: 0.9228 - val_loss: 0.2425 - val_acc: 0.9142\n",
      "Epoch 12/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9267\n",
      "Epoch 12: val_loss did not improve from 0.24189\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2115 - acc: 0.9268 - val_loss: 0.2428 - val_acc: 0.9128\n",
      "Epoch 13/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9248\n",
      "Epoch 13: val_loss improved from 0.24189 to 0.24141, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2131 - acc: 0.9246 - val_loss: 0.2414 - val_acc: 0.9156\n",
      "Epoch 14/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9256\n",
      "Epoch 14: val_loss did not improve from 0.24141\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.2099 - acc: 0.9256 - val_loss: 0.2416 - val_acc: 0.9147\n",
      "Epoch 15/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9261\n",
      "Epoch 15: val_loss did not improve from 0.24141\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2087 - acc: 0.9260 - val_loss: 0.2418 - val_acc: 0.9142\n",
      "Epoch 16/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9277\n",
      "Epoch 16: val_loss did not improve from 0.24141\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2069 - acc: 0.9276 - val_loss: 0.2417 - val_acc: 0.9147\n",
      "Epoch 17/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9265\n",
      "Epoch 17: val_loss improved from 0.24141 to 0.24129, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2064 - acc: 0.9266 - val_loss: 0.2413 - val_acc: 0.9152\n",
      "Epoch 18/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9265\n",
      "Epoch 18: val_loss did not improve from 0.24129\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2058 - acc: 0.9266 - val_loss: 0.2415 - val_acc: 0.9152\n",
      "Epoch 19/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9288\n",
      "Epoch 19: val_loss did not improve from 0.24129\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2044 - acc: 0.9288 - val_loss: 0.2415 - val_acc: 0.9142\n",
      "Epoch 20/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2056 - acc: 0.9264\n",
      "Epoch 20: val_loss improved from 0.24129 to 0.24116, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2056 - acc: 0.9264 - val_loss: 0.2412 - val_acc: 0.9142\n",
      "Epoch 21/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9287\n",
      "Epoch 21: val_loss did not improve from 0.24116\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2012 - acc: 0.9288 - val_loss: 0.2413 - val_acc: 0.9138\n",
      "Epoch 22/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9286\n",
      "Epoch 22: val_loss improved from 0.24116 to 0.24098, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2017 - acc: 0.9286 - val_loss: 0.2410 - val_acc: 0.9142\n",
      "Epoch 23/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9294\n",
      "Epoch 23: val_loss did not improve from 0.24098\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2000 - acc: 0.9298 - val_loss: 0.2414 - val_acc: 0.9138\n",
      "Epoch 24/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9297\n",
      "Epoch 24: val_loss did not improve from 0.24098\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2016 - acc: 0.9299 - val_loss: 0.2413 - val_acc: 0.9133\n",
      "Epoch 25/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9291\n",
      "Epoch 25: val_loss did not improve from 0.24098\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1984 - acc: 0.9290 - val_loss: 0.2412 - val_acc: 0.9133\n",
      "Epoch 26/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9300\n",
      "Epoch 26: val_loss did not improve from 0.24098\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1983 - acc: 0.9299 - val_loss: 0.2411 - val_acc: 0.9147\n",
      "Epoch 27/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9293\n",
      "Epoch 27: val_loss did not improve from 0.24098\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1985 - acc: 0.9290 - val_loss: 0.2413 - val_acc: 0.9114\n",
      "133/133 [==============================] - 1s 5ms/step\n",
      "0.8826612867856674 0.5829752686160816 0.9160359508041628 0.48920863309352514\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9338\n",
      "Epoch 1: val_loss improved from inf to 0.18380, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 6s 19ms/step - loss: 0.2175 - acc: 0.9335 - val_loss: 0.1838 - val_acc: 0.9458\n",
      "Epoch 2/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9421\n",
      "Epoch 2: val_loss improved from 0.18380 to 0.18018, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.1840 - acc: 0.9421 - val_loss: 0.1802 - val_acc: 0.9458\n",
      "Epoch 3/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9437\n",
      "Epoch 3: val_loss improved from 0.18018 to 0.17819, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1718 - acc: 0.9438 - val_loss: 0.1782 - val_acc: 0.9420\n",
      "Epoch 4/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9463\n",
      "Epoch 4: val_loss did not improve from 0.17819\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1653 - acc: 0.9462 - val_loss: 0.1782 - val_acc: 0.9463\n",
      "Epoch 5/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9475\n",
      "Epoch 5: val_loss improved from 0.17819 to 0.17635, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 5s 21ms/step - loss: 0.1613 - acc: 0.9474 - val_loss: 0.1764 - val_acc: 0.9430\n",
      "Epoch 6/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9494\n",
      "Epoch 6: val_loss improved from 0.17635 to 0.17530, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1586 - acc: 0.9495 - val_loss: 0.1753 - val_acc: 0.9434\n",
      "Epoch 7/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9483\n",
      "Epoch 7: val_loss did not improve from 0.17530\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1546 - acc: 0.9484 - val_loss: 0.1755 - val_acc: 0.9425\n",
      "Epoch 8/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9507\n",
      "Epoch 8: val_loss did not improve from 0.17530\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1531 - acc: 0.9507 - val_loss: 0.1756 - val_acc: 0.9430\n",
      "Epoch 9/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9507\n",
      "Epoch 9: val_loss did not improve from 0.17530\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1505 - acc: 0.9507 - val_loss: 0.1765 - val_acc: 0.9449\n",
      "Epoch 10/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9506\n",
      "Epoch 10: val_loss improved from 0.17530 to 0.17475, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1482 - acc: 0.9505 - val_loss: 0.1747 - val_acc: 0.9425\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1477 - acc: 0.9505\n",
      "Epoch 11: val_loss did not improve from 0.17475\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1477 - acc: 0.9505 - val_loss: 0.1752 - val_acc: 0.9444\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1470 - acc: 0.9497\n",
      "Epoch 12: val_loss did not improve from 0.17475\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.1470 - acc: 0.9497 - val_loss: 0.1752 - val_acc: 0.9434\n",
      "Epoch 13/100\n",
      "230/233 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9517\n",
      "Epoch 13: val_loss did not improve from 0.17475\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1443 - acc: 0.9516 - val_loss: 0.1753 - val_acc: 0.9416\n",
      "Epoch 14/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9513\n",
      "Epoch 14: val_loss did not improve from 0.17475\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1441 - acc: 0.9511 - val_loss: 0.1750 - val_acc: 0.9444\n",
      "Epoch 15/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.9516\n",
      "Epoch 15: val_loss did not improve from 0.17475\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1435 - acc: 0.9516 - val_loss: 0.1749 - val_acc: 0.9425\n",
      "133/133 [==============================] - 1s 4ms/step\n",
      "0.8909661603890239 0.53315684662978 0.9406338694418165 0.4602150537634408\n",
      "Embedding:  fasttext\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.8964\n",
      "Epoch 1: val_loss improved from inf to 0.25268, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 6s 19ms/step - loss: 0.3048 - acc: 0.8962 - val_loss: 0.2527 - val_acc: 0.9109\n",
      "Epoch 2/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9126\n",
      "Epoch 2: val_loss improved from 0.25268 to 0.24760, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 17ms/step - loss: 0.2525 - acc: 0.9127 - val_loss: 0.2476 - val_acc: 0.9142\n",
      "Epoch 3/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9172\n",
      "Epoch 3: val_loss improved from 0.24760 to 0.24448, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 19ms/step - loss: 0.2398 - acc: 0.9172 - val_loss: 0.2445 - val_acc: 0.9142\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2322 - acc: 0.9183\n",
      "Epoch 4: val_loss improved from 0.24448 to 0.24375, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2322 - acc: 0.9183 - val_loss: 0.2438 - val_acc: 0.9128\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2246 - acc: 0.9213\n",
      "Epoch 5: val_loss improved from 0.24375 to 0.24293, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2246 - acc: 0.9213 - val_loss: 0.2429 - val_acc: 0.9142\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2222 - acc: 0.9206\n",
      "Epoch 6: val_loss improved from 0.24293 to 0.24226, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2222 - acc: 0.9206 - val_loss: 0.2423 - val_acc: 0.9133\n",
      "Epoch 7/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9235\n",
      "Epoch 7: val_loss improved from 0.24226 to 0.24216, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2186 - acc: 0.9235 - val_loss: 0.2422 - val_acc: 0.9119\n",
      "Epoch 8/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9224\n",
      "Epoch 8: val_loss improved from 0.24216 to 0.24156, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2172 - acc: 0.9226 - val_loss: 0.2416 - val_acc: 0.9138\n",
      "Epoch 9/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9241\n",
      "Epoch 9: val_loss improved from 0.24156 to 0.24091, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2142 - acc: 0.9241 - val_loss: 0.2409 - val_acc: 0.9166\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2103 - acc: 0.9261\n",
      "Epoch 10: val_loss improved from 0.24091 to 0.24059, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2103 - acc: 0.9261 - val_loss: 0.2406 - val_acc: 0.9161\n",
      "Epoch 11/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9258\n",
      "Epoch 11: val_loss did not improve from 0.24059\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.2102 - acc: 0.9258 - val_loss: 0.2407 - val_acc: 0.9161\n",
      "Epoch 12/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9252\n",
      "Epoch 12: val_loss improved from 0.24059 to 0.24014, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2091 - acc: 0.9251 - val_loss: 0.2401 - val_acc: 0.9156\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2064 - acc: 0.9259\n",
      "Epoch 13: val_loss did not improve from 0.24014\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.2064 - acc: 0.9259 - val_loss: 0.2409 - val_acc: 0.9161\n",
      "Epoch 14/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9276\n",
      "Epoch 14: val_loss did not improve from 0.24014\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2061 - acc: 0.9276 - val_loss: 0.2405 - val_acc: 0.9138\n",
      "Epoch 15/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9269\n",
      "Epoch 15: val_loss improved from 0.24014 to 0.24005, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2043 - acc: 0.9269 - val_loss: 0.2400 - val_acc: 0.9142\n",
      "Epoch 16/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9269\n",
      "Epoch 16: val_loss improved from 0.24005 to 0.24002, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.2045 - acc: 0.9268 - val_loss: 0.2400 - val_acc: 0.9156\n",
      "Epoch 17/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2020 - acc: 0.9274\n",
      "Epoch 17: val_loss did not improve from 0.24002\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2020 - acc: 0.9274 - val_loss: 0.2403 - val_acc: 0.9152\n",
      "Epoch 18/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9283\n",
      "Epoch 18: val_loss did not improve from 0.24002\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.2005 - acc: 0.9284 - val_loss: 0.2403 - val_acc: 0.9138\n",
      "Epoch 19/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9286\n",
      "Epoch 19: val_loss did not improve from 0.24002\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.2010 - acc: 0.9286 - val_loss: 0.2401 - val_acc: 0.9147\n",
      "Epoch 20/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9284\n",
      "Epoch 20: val_loss did not improve from 0.24002\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.2000 - acc: 0.9285 - val_loss: 0.2402 - val_acc: 0.9142\n",
      "Epoch 21/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9289\n",
      "Epoch 21: val_loss did not improve from 0.24002\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1985 - acc: 0.9289 - val_loss: 0.2403 - val_acc: 0.9142\n",
      "133/133 [==============================] - 1s 4ms/step\n",
      "0.8826092422846951 0.5828869447383906 0.9160359508041628 0.48175182481751827\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9330\n",
      "Epoch 1: val_loss improved from inf to 0.18611, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 6s 19ms/step - loss: 0.2257 - acc: 0.9329 - val_loss: 0.1861 - val_acc: 0.9411\n",
      "Epoch 2/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9408\n",
      "Epoch 2: val_loss improved from 0.18611 to 0.18103, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1835 - acc: 0.9409 - val_loss: 0.1810 - val_acc: 0.9406\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1737 - acc: 0.9437\n",
      "Epoch 3: val_loss improved from 0.18103 to 0.17730, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1737 - acc: 0.9437 - val_loss: 0.1773 - val_acc: 0.9444\n",
      "Epoch 4/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9455\n",
      "Epoch 4: val_loss did not improve from 0.17730\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1675 - acc: 0.9456 - val_loss: 0.1775 - val_acc: 0.9430\n",
      "Epoch 5/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9461\n",
      "Epoch 5: val_loss improved from 0.17730 to 0.17620, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 5s 21ms/step - loss: 0.1622 - acc: 0.9462 - val_loss: 0.1762 - val_acc: 0.9449\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1579 - acc: 0.9488\n",
      "Epoch 6: val_loss did not improve from 0.17620\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1579 - acc: 0.9488 - val_loss: 0.1766 - val_acc: 0.9420\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1559 - acc: 0.9485\n",
      "Epoch 7: val_loss improved from 0.17620 to 0.17559, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1559 - acc: 0.9485 - val_loss: 0.1756 - val_acc: 0.9425\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1541 - acc: 0.9500\n",
      "Epoch 8: val_loss did not improve from 0.17559\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1541 - acc: 0.9500 - val_loss: 0.1757 - val_acc: 0.9434\n",
      "Epoch 9/100\n",
      "232/233 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9500\n",
      "Epoch 9: val_loss did not improve from 0.17559\n",
      "233/233 [==============================] - 5s 19ms/step - loss: 0.1512 - acc: 0.9502 - val_loss: 0.1760 - val_acc: 0.9425\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1501 - acc: 0.9516\n",
      "Epoch 10: val_loss did not improve from 0.17559\n",
      "233/233 [==============================] - 4s 18ms/step - loss: 0.1501 - acc: 0.9516 - val_loss: 0.1759 - val_acc: 0.9420\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1475 - acc: 0.9519\n",
      "Epoch 11: val_loss did not improve from 0.17559\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1475 - acc: 0.9519 - val_loss: 0.1758 - val_acc: 0.9425\n",
      "Epoch 12/100\n",
      "231/233 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9508\n",
      "Epoch 12: val_loss did not improve from 0.17559\n",
      "233/233 [==============================] - 4s 19ms/step - loss: 0.1476 - acc: 0.9505 - val_loss: 0.1756 - val_acc: 0.9420\n",
      "133/133 [==============================] - 1s 4ms/step\n",
      "0.8862600113987462 0.5258214061786566 0.9420529801324503 0.45916114790286977\n",
      "Hidden unit:  256\n",
      "Embedding:  word2vec\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2848 - acc: 0.9026\n",
      "Epoch 1: val_loss improved from inf to 0.25158, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 11s 41ms/step - loss: 0.2848 - acc: 0.9026 - val_loss: 0.2516 - val_acc: 0.9114\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2394 - acc: 0.9168\n",
      "Epoch 2: val_loss improved from 0.25158 to 0.24856, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2394 - acc: 0.9168 - val_loss: 0.2486 - val_acc: 0.9123\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2258 - acc: 0.9206\n",
      "Epoch 3: val_loss improved from 0.24856 to 0.24122, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2258 - acc: 0.9206 - val_loss: 0.2412 - val_acc: 0.9142\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2179 - acc: 0.9222\n",
      "Epoch 4: val_loss improved from 0.24122 to 0.23968, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2179 - acc: 0.9222 - val_loss: 0.2397 - val_acc: 0.9156\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2134 - acc: 0.9235\n",
      "Epoch 5: val_loss improved from 0.23968 to 0.23862, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2134 - acc: 0.9235 - val_loss: 0.2386 - val_acc: 0.9156\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2075 - acc: 0.9258\n",
      "Epoch 6: val_loss did not improve from 0.23862\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2075 - acc: 0.9258 - val_loss: 0.2391 - val_acc: 0.9142\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.9270\n",
      "Epoch 7: val_loss did not improve from 0.23862\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2055 - acc: 0.9270 - val_loss: 0.2415 - val_acc: 0.9133\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2007 - acc: 0.9284\n",
      "Epoch 8: val_loss did not improve from 0.23862\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2007 - acc: 0.9284 - val_loss: 0.2403 - val_acc: 0.9133\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1988 - acc: 0.9297\n",
      "Epoch 9: val_loss did not improve from 0.23862\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1988 - acc: 0.9297 - val_loss: 0.2417 - val_acc: 0.9133\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1964 - acc: 0.9300\n",
      "Epoch 10: val_loss did not improve from 0.23862\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1964 - acc: 0.9300 - val_loss: 0.2408 - val_acc: 0.9123\n",
      "133/133 [==============================] - 2s 10ms/step\n",
      "0.8818086926012019 0.5759955735714865 0.9141438032166509 0.43894899536321486\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2172 - acc: 0.9346\n",
      "Epoch 1: val_loss improved from inf to 0.18272, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 11s 42ms/step - loss: 0.2172 - acc: 0.9346 - val_loss: 0.1827 - val_acc: 0.9420\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9429\n",
      "Epoch 2: val_loss improved from 0.18272 to 0.18025, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1753 - acc: 0.9429 - val_loss: 0.1803 - val_acc: 0.9425\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1642 - acc: 0.9470\n",
      "Epoch 3: val_loss improved from 0.18025 to 0.17553, saving model to avg-word2vec-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1642 - acc: 0.9470 - val_loss: 0.1755 - val_acc: 0.9434\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1569 - acc: 0.9479\n",
      "Epoch 4: val_loss did not improve from 0.17553\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1569 - acc: 0.9479 - val_loss: 0.1765 - val_acc: 0.9453\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1515 - acc: 0.9497\n",
      "Epoch 5: val_loss did not improve from 0.17553\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1515 - acc: 0.9497 - val_loss: 0.1783 - val_acc: 0.9425\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1489 - acc: 0.9497\n",
      "Epoch 6: val_loss did not improve from 0.17553\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1489 - acc: 0.9497 - val_loss: 0.1765 - val_acc: 0.9444\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1449 - acc: 0.9509\n",
      "Epoch 7: val_loss did not improve from 0.17553\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1449 - acc: 0.9509 - val_loss: 0.1785 - val_acc: 0.9411\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1416 - acc: 0.9517\n",
      "Epoch 8: val_loss did not improve from 0.17553\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1416 - acc: 0.9517 - val_loss: 0.1778 - val_acc: 0.9444\n",
      "133/133 [==============================] - 1s 8ms/step\n",
      "0.8857034059586779 0.513000279542809 0.9411069063386944 0.42494226327944573\n",
      "Embedding:  fasttext\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2805 - acc: 0.9060\n",
      "Epoch 1: val_loss improved from inf to 0.24905, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 11s 41ms/step - loss: 0.2805 - acc: 0.9060 - val_loss: 0.2491 - val_acc: 0.9114\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2393 - acc: 0.9170\n",
      "Epoch 2: val_loss improved from 0.24905 to 0.24351, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2393 - acc: 0.9170 - val_loss: 0.2435 - val_acc: 0.9128\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2272 - acc: 0.9217\n",
      "Epoch 3: val_loss did not improve from 0.24351\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2272 - acc: 0.9217 - val_loss: 0.2465 - val_acc: 0.9086\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2182 - acc: 0.9222\n",
      "Epoch 4: val_loss improved from 0.24351 to 0.24166, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2182 - acc: 0.9222 - val_loss: 0.2417 - val_acc: 0.9138\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2113 - acc: 0.9249\n",
      "Epoch 5: val_loss did not improve from 0.24166\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2113 - acc: 0.9249 - val_loss: 0.2422 - val_acc: 0.9119\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2067 - acc: 0.9275\n",
      "Epoch 6: val_loss improved from 0.24166 to 0.24133, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2067 - acc: 0.9275 - val_loss: 0.2413 - val_acc: 0.9090\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2027 - acc: 0.9272\n",
      "Epoch 7: val_loss improved from 0.24133 to 0.24032, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2027 - acc: 0.9272 - val_loss: 0.2403 - val_acc: 0.9152\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2008 - acc: 0.9284\n",
      "Epoch 8: val_loss did not improve from 0.24032\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2008 - acc: 0.9284 - val_loss: 0.2407 - val_acc: 0.9128\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1973 - acc: 0.9289\n",
      "Epoch 9: val_loss improved from 0.24032 to 0.24000, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1973 - acc: 0.9289 - val_loss: 0.2400 - val_acc: 0.9119\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1951 - acc: 0.9293\n",
      "Epoch 10: val_loss did not improve from 0.24000\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1951 - acc: 0.9293 - val_loss: 0.2410 - val_acc: 0.9114\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1943 - acc: 0.9305\n",
      "Epoch 11: val_loss did not improve from 0.24000\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1943 - acc: 0.9305 - val_loss: 0.2420 - val_acc: 0.9123\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1925 - acc: 0.9313\n",
      "Epoch 12: val_loss did not improve from 0.24000\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1925 - acc: 0.9313 - val_loss: 0.2420 - val_acc: 0.9114\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1895 - acc: 0.9331\n",
      "Epoch 13: val_loss did not improve from 0.24000\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1895 - acc: 0.9331 - val_loss: 0.2423 - val_acc: 0.9128\n",
      "Epoch 14/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1884 - acc: 0.9328\n",
      "Epoch 14: val_loss did not improve from 0.24000\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1884 - acc: 0.9328 - val_loss: 0.2409 - val_acc: 0.9138\n",
      "133/133 [==============================] - 2s 10ms/step\n",
      "0.8803549551920238 0.5793658715213083 0.9139072847682119 0.48295454545454547\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.2235 - acc: 0.9334\n",
      "Epoch 1: val_loss improved from inf to 0.18443, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 11s 42ms/step - loss: 0.2235 - acc: 0.9334 - val_loss: 0.1844 - val_acc: 0.9387\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1779 - acc: 0.9436\n",
      "Epoch 2: val_loss improved from 0.18443 to 0.18022, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1779 - acc: 0.9436 - val_loss: 0.1802 - val_acc: 0.9434\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1653 - acc: 0.9466\n",
      "Epoch 3: val_loss improved from 0.18022 to 0.17618, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1653 - acc: 0.9466 - val_loss: 0.1762 - val_acc: 0.9416\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1584 - acc: 0.9486\n",
      "Epoch 4: val_loss did not improve from 0.17618\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1584 - acc: 0.9486 - val_loss: 0.1772 - val_acc: 0.9420\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1553 - acc: 0.9486\n",
      "Epoch 5: val_loss did not improve from 0.17618\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1553 - acc: 0.9486 - val_loss: 0.1765 - val_acc: 0.9449\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1504 - acc: 0.9514\n",
      "Epoch 6: val_loss did not improve from 0.17618\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1504 - acc: 0.9514 - val_loss: 0.1766 - val_acc: 0.9416\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1472 - acc: 0.9513\n",
      "Epoch 7: val_loss did not improve from 0.17618\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1472 - acc: 0.9513 - val_loss: 0.1769 - val_acc: 0.9416\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1430 - acc: 0.9531\n",
      "Epoch 8: val_loss improved from 0.17618 to 0.17558, saving model to avg-fasttext-mort_icu-best_model.hdf5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1430 - acc: 0.9531 - val_loss: 0.1756 - val_acc: 0.9425\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1416 - acc: 0.9523\n",
      "Epoch 9: val_loss did not improve from 0.17558\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1416 - acc: 0.9523 - val_loss: 0.1766 - val_acc: 0.9411\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1402 - acc: 0.9527\n",
      "Epoch 10: val_loss did not improve from 0.17558\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1402 - acc: 0.9527 - val_loss: 0.1769 - val_acc: 0.9425\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1374 - acc: 0.9545\n",
      "Epoch 11: val_loss did not improve from 0.17558\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1374 - acc: 0.9545 - val_loss: 0.1770 - val_acc: 0.9402\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1359 - acc: 0.9538\n",
      "Epoch 12: val_loss did not improve from 0.17558\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1359 - acc: 0.9538 - val_loss: 0.1777 - val_acc: 0.9430\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - ETA: 0s - loss: 0.1340 - acc: 0.9542\n",
      "Epoch 13: val_loss did not improve from 0.17558\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.1340 - acc: 0.9542 - val_loss: 0.1769 - val_acc: 0.9406\n",
      "133/133 [==============================] - 2s 10ms/step\n",
      "0.8868232827722284 0.5309060926747315 0.9415799432355724 0.4595185995623633\n"
     ]
    }
   ],
   "source": [
    "embedding_types = ['word2vec', 'fasttext'] #, 'concat']\n",
    "embedding_dict = [ner_word2vec, ner_fasttext] #, ner_concat]\n",
    "target_problems = ['mort_hosp', 'mort_icu'] #, 'los_3', 'los_7']\n",
    "\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size = 64\n",
    "iter_num = 2\n",
    "unit_sizes = [128, 256]\n",
    "\n",
    "#layers = [\"LSTM\", \"GRU\"]\n",
    "layers = [\"GRU\"]\n",
    "for each_layer in layers:\n",
    "    print (\"Layer: \", each_layer)\n",
    "    for each_unit_size in unit_sizes:\n",
    "        print (\"Hidden unit: \", each_unit_size)\n",
    "\n",
    "        for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n",
    "            print (\"Embedding: \", embed_name)\n",
    "            print(\"=============================\")\n",
    "\n",
    "            temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n",
    "            temp_dev_ner = dict((k, ner_word2vec[k]) for k in dev_ids)\n",
    "            temp_test_ner = dict((k, ner_word2vec[k]) for k in test_ids)\n",
    "\n",
    "            x_train_ner = create_dataset(temp_train_ner)\n",
    "            x_dev_ner = create_dataset(temp_dev_ner)\n",
    "            x_test_ner = create_dataset(temp_test_ner)\n",
    "\n",
    "\n",
    "            for iteration in range(1, iter_num):\n",
    "                print (\"Iteration number: \", iteration)\n",
    "\n",
    "                for each_problem in target_problems:\n",
    "                    print (\"Problem type: \", each_problem)\n",
    "                    print (\"__________________\")\n",
    "\n",
    "                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
    "                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "                    checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n",
    "                        save_best_only=True, mode='min', period=1)\n",
    "\n",
    "\n",
    "                    callbacks = [early_stopping_monitor, checkpoint]\n",
    "\n",
    "                    model = avg_ner_model(each_layer, each_unit_size, embed_name)\n",
    "                    \n",
    "                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n",
    "                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, \n",
    "                              batch_size=batch_size )\n",
    "\n",
    "                    model.load_weights(best_model_name)\n",
    "\n",
    "                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n",
    "                    \n",
    "                    save_scores_multi_avg(predictions, probs, y_test[each_problem],\n",
    "                                embed_name, each_problem, iteration, each_unit_size,\n",
    "                                each_layer, type_of_ner)\n",
    "\n",
    "\n",
    "                    # reset_keras(model)\n",
    "                    #del model\n",
    "                    sess = get_session()\n",
    "                    clear_session()\n",
    "                    sess.close()\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
